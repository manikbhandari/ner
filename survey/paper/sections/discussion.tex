\section{Discussion}
\label{sec:discussion}

\begin{enumerate}
	\item Preliminary experiments suggest that training BERT from scratch on a small corpus (~20K sentences) is bad. This is a little surprising (but kind of expected) because the vocabulary size of this corpus is only around 10K.
	\item Next in performance is Pretrained BERT without any language modeling fine tuning. Best so far (apart form SOTA) is BERT with language model fine tuning to domain specific corpus.
\end{enumerate}
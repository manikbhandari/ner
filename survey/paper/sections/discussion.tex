\section{Discussion}
\label{sec:discussion}

\begin{enumerate}
	\item Preliminary experiments suggest that training BERT from scratch on a small corpus (~20K sentences) is bad. This was expected but I was hoping it would perform slightly better because the vocabulary size of this corpus is only around 10K.
	\item Next in performance is Pretrained BERT without any language modeling fine tuning. 
	\item Best so far (apart form SOTA) is BERT with language model fine tuning to domain specific corpus.
\end{enumerate}